{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment\n",
    "path_to_file = \"../data/Component_Faults_Data.csv\"\n",
    "df = pd.read_csv(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment\n",
    "x = df.iloc[:, :48].values\n",
    "y = df[\"class\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newer Solution for testing NN-Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports for class\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt    \n",
    "import itertools\n",
    "        \n",
    "\n",
    "\"\"\" \n",
    "    The following class helps in streamlining the building, training, and testing of a Neural Network (NN)\n",
    "    With it, we can see the effect on the result for changing a single parameter \n",
    "    So we only have to provide the name of the parameter we have to change, and the values we want to test it with\n",
    "\"\"\"\n",
    "class NeuralNetworkParameterTester:\n",
    "\n",
    "    # The config holds all the changeable parameters for building, training and testing the NN\n",
    "    config = None\n",
    "    result = []  # Result of the test\n",
    "    \n",
    "    def __init__(self, set_default_config=True):\n",
    "        \n",
    "        if set_default_config:\n",
    "            default_config = {\n",
    "                'number_of_hidden_layers': [1],\n",
    "                'number_of_units_per_hidden_layer': [10],\n",
    "                'epochs': [100],\n",
    "                'batch_size': [64],\n",
    "                'activation_function': ['relu'],\n",
    "                'loss_function': ['categorical_crossentropy'],\n",
    "                'optimizer': ['sgd']}\n",
    "\n",
    "            self.config = default_config\n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        for config in self.__build_all_config_combinations():\n",
    "            model = self.__build(config)\n",
    "            trained_model, history = self.__train(config, model)\n",
    "            accuracy = self.__test(trained_model)\n",
    "\n",
    "            # save result\n",
    "            self.result.append({'params': config.copy(), \n",
    "                                \"result\": {'model': trained_model, \n",
    "                                           'accuracy': accuracy}})\n",
    "            \n",
    "    def __build_all_config_combinations(self):\n",
    "\n",
    "        raw_combinations = list(itertools.product(*(self.config[parameter] for parameter in self.config)))\n",
    "\n",
    "        config_combinations_list = []\n",
    "        for combination in raw_combinations:\n",
    "            c = {\n",
    "                'number_of_hidden_layers': combination[0],\n",
    "                'number_of_units_per_hidden_layer': combination[1],\n",
    "                'epochs': combination[2],\n",
    "                'batch_size': combination[3],\n",
    "                'activation_function': combination[4],\n",
    "                'loss_function': combination[5],\n",
    "                'optimizer': combination[6]}\n",
    "            \n",
    "            config_combinations_list.append(c)\n",
    "        \n",
    "        return config_combinations_list\n",
    "\n",
    "    def __build(self, test_config):\n",
    "\n",
    "        # Sequential model (Basic NN)\n",
    "        model = Sequential()        \n",
    "        # Building of input layer\n",
    "        model.add(Dense(test_config['number_of_units_per_hidden_layer'], \n",
    "                        input_dim=48, \n",
    "                        activation=test_config['activation_function']))\n",
    "        # Building of hidden layer(s)\n",
    "        for i in range(test_config['number_of_hidden_layers']):\n",
    "            model.add(Dense(test_config['number_of_units_per_hidden_layer'], \n",
    "                            activation=test_config['activation_function']))\n",
    "        # Building of output layer\n",
    "        model.add(Dense(11, activation=\"softmax\"))\n",
    "        # ?\n",
    "        model.compile(loss=test_config['loss_function'], \n",
    "                      optimizer=test_config['optimizer'], \n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def __train(self, test_config, model):\n",
    "        # xtrain and ytrain is the data from preprocessing\n",
    "        history = model.fit(x_train, \n",
    "                            y_train, \n",
    "                            epochs=test_config['epochs'], \n",
    "                            batch_size=test_config['batch_size'])  \n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    \n",
    "    # TODO: look at this method more closely. So far just copy paste\n",
    "    @staticmethod\n",
    "    def __test(model):\n",
    "        y_pred = model.predict(x_test)\n",
    "        # Converting predictions to label\n",
    "        pred = list()\n",
    "        for i in range(len(y_pred)):\n",
    "            pred.append(np.argmax(y_pred[i]))\n",
    "        # Converting one hot encoded test label to label\n",
    "        test = list()\n",
    "        for i in range(len(y_test)):\n",
    "            test.append(np.argmax(y_test[i]))\n",
    "\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score(pred, test)\n",
    "        return accuracy\n",
    "                                    \n",
    "                                    \n",
    "    # Setter methods for setting single parameters\n",
    "    def set_number_of_hidden_layers(self, val):\n",
    "        if isinstance(val, list):\n",
    "            self.config[\"number_of_hidden_layers\"] = val\n",
    "        else:\n",
    "            self.config[\"number_of_hidden_layers\"] = [val]\n",
    "        \n",
    "    def set_number_of_units_per_hidden_layer(self, val):\n",
    "        if isinstance(val, list):\n",
    "            self.config[\"number_of_units_per_hidden_layer\"] = val\n",
    "        else:\n",
    "            self.config[\"number_of_units_per_hidden_layer\"] = [val]\n",
    "        \n",
    "    def set_activation_function(self, val):\n",
    "        if isinstance(val, list):\n",
    "            self.config[\"activation_function\"] = val\n",
    "        else:\n",
    "            self.config[\"activation_function\"] = [val]\n",
    "        \n",
    "    def set_epochs(self, val):\n",
    "        if isinstance(val, list):\n",
    "            self.config[\"epochs\"] = val\n",
    "        else:\n",
    "            self.config[\"epochs\"] = [val]\n",
    "        \n",
    "    def set_batch_size(self, val):\n",
    "        if isinstance(val, list):\n",
    "            self.config[\"batch_size\"] = val\n",
    "        else:\n",
    "            self.config[\"batch_size\"] = [val]\n",
    "        \n",
    "    def set_loss_function(self, val):\n",
    "        if isinstance(val, list):\n",
    "            self.config[\"loss_function\"] = val\n",
    "        else:\n",
    "            self.config[\"loss_function\"] = [val]\n",
    "        \n",
    "    def set_optimizer(self, val):\n",
    "        if isinstance(val, list):\n",
    "            self.config[\"optimizer\"] = val\n",
    "        else:\n",
    "            self.config[\"optimizer\"] = [val]\n",
    "        \n",
    "    # This method is helpful if you just want to reset the changes you made to the parameters\n",
    "    def reset_config(self):\n",
    "        default_config = {\n",
    "            'number_of_hidden_layers': [1],\n",
    "            'number_of_units_per_hidden_layer': [10],\n",
    "            'epochs': [100],\n",
    "            'batch_size': [64],\n",
    "            'activation_function': ['relu'],\n",
    "            'loss_function': ['categorical_crossentropy'],\n",
    "            'optimizer': ['sgd']}\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "    def __get_number_of_params_with_multiple_vals(self):\n",
    "        number_of_params_with_multiple_vals = 0 \n",
    "        for key in self.config.keys():\n",
    "            if isinstance(self.config[key], list):\n",
    "                number_of_params_with_multiple_vals += 1\n",
    "        return number_of_params_with_multiple_vals\n",
    "        \n",
    "    # This method plots the result without needing any input from the user\n",
    "    def plot_result(self):\n",
    "        if self.__get_number_of_params_with_multiple_vals() <= 1:\n",
    "            self.__plot_2d()\n",
    "        else:\n",
    "            print(\"Not supported\")\n",
    "\n",
    "    def __plot_2d(self):\n",
    "\n",
    "        # TODO: improve plot\n",
    "\n",
    "        test_param_val = [element['test_param_val'] for element in self.result]\n",
    "        accuracy_result = [element['result']['accuracy'] for element in self.result]\n",
    "\n",
    "        plt.scatter(test_param_val, accuracy_result)\n",
    "        plt.plot(test_param_val, accuracy_result, linestyle='--')\n",
    "        plt.title(\"Accuracy per \" + \"'\" + self.test_param_name + \"'\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(self.test_param_name)\n",
    "        plt.show()\n",
    "        # TODO : plot as table\n",
    "        print('\"test_param_val\":\\t', test_param_val)\n",
    "        print('\"accuracy_result\":\\t', accuracy_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NN with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nnpt = NeuralNetworkParameterTester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnpt.set_number_of_hidden_layers([2, 5, 10])\n",
    "nnpt.set_number_of_units_per_hidden_layer([2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number_of_hidden_layers': [2, 5, 10],\n",
       " 'number_of_units_per_hidden_layer': [2, 4],\n",
       " 'epochs': [100],\n",
       " 'batch_size': [64],\n",
       " 'activation_function': ['relu'],\n",
       " 'loss_function': ['categorical_crossentropy'],\n",
       " 'optimizer': ['sgd']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnpt.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "26329/26329 [==============================] - 1s 27us/step - loss: 2.4281 - acc: 0.0858\n",
      "Epoch 2/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 2.3980 - acc: 0.0885\n",
      "Epoch 3/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 2.3959 - acc: 0.0904\n",
      "Epoch 4/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 2.3939 - acc: 0.1001\n",
      "Epoch 5/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 2.3907 - acc: 0.1108\n",
      "Epoch 6/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 2.3839 - acc: 0.1139\n",
      "Epoch 7/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 2.3674 - acc: 0.1344\n",
      "Epoch 8/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 2.3273 - acc: 0.1678\n",
      "Epoch 9/100\n",
      "26329/26329 [==============================] - ETA: 0s - loss: 2.2343 - acc: 0.165 - 0s 15us/step - loss: 2.2299 - acc: 0.1645\n",
      "Epoch 10/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 2.0832 - acc: 0.1678\n",
      "Epoch 11/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.9808 - acc: 0.1853\n",
      "Epoch 12/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.9182 - acc: 0.1950\n",
      "Epoch 13/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.8746 - acc: 0.2098\n",
      "Epoch 14/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.8423 - acc: 0.2241\n",
      "Epoch 15/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.8164 - acc: 0.2343\n",
      "Epoch 16/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.7952 - acc: 0.2443\n",
      "Epoch 17/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.7771 - acc: 0.2553\n",
      "Epoch 18/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.7616 - acc: 0.2672\n",
      "Epoch 19/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.7476 - acc: 0.2685\n",
      "Epoch 20/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.7348 - acc: 0.2767\n",
      "Epoch 21/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.7243 - acc: 0.2809\n",
      "Epoch 22/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.7146 - acc: 0.2859\n",
      "Epoch 23/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.7056 - acc: 0.2865\n",
      "Epoch 24/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.6972 - acc: 0.2913\n",
      "Epoch 25/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.6893 - acc: 0.2952\n",
      "Epoch 26/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.6816 - acc: 0.2976\n",
      "Epoch 27/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.6734 - acc: 0.3000\n",
      "Epoch 28/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.6651 - acc: 0.3049\n",
      "Epoch 29/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.6543 - acc: 0.3124\n",
      "Epoch 30/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.6414 - acc: 0.3207\n",
      "Epoch 31/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.6226 - acc: 0.3292\n",
      "Epoch 32/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.6070 - acc: 0.3331\n",
      "Epoch 33/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5935 - acc: 0.3348\n",
      "Epoch 34/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5817 - acc: 0.3374\n",
      "Epoch 35/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5709 - acc: 0.3411\n",
      "Epoch 36/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5617 - acc: 0.3361\n",
      "Epoch 37/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.5535 - acc: 0.3406\n",
      "Epoch 38/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5456 - acc: 0.3402\n",
      "Epoch 39/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5393 - acc: 0.3446\n",
      "Epoch 40/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5332 - acc: 0.3439\n",
      "Epoch 41/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.5269 - acc: 0.3423\n",
      "Epoch 42/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5224 - acc: 0.3448\n",
      "Epoch 43/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5171 - acc: 0.3478\n",
      "Epoch 44/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5134 - acc: 0.3458\n",
      "Epoch 45/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5086 - acc: 0.3481\n",
      "Epoch 46/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5048 - acc: 0.3529\n",
      "Epoch 47/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.5014 - acc: 0.3499\n",
      "Epoch 48/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4979 - acc: 0.3500\n",
      "Epoch 49/100\n",
      "26329/26329 [==============================] - ETA: 0s - loss: 1.4953 - acc: 0.352 - 0s 14us/step - loss: 1.4952 - acc: 0.3522\n",
      "Epoch 50/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4911 - acc: 0.3543\n",
      "Epoch 51/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4888 - acc: 0.3535\n",
      "Epoch 52/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4854 - acc: 0.3512\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4827 - acc: 0.3565\n",
      "Epoch 54/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4800 - acc: 0.3552\n",
      "Epoch 55/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4767 - acc: 0.3532\n",
      "Epoch 56/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4749 - acc: 0.3589\n",
      "Epoch 57/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4727 - acc: 0.3575\n",
      "Epoch 58/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4697 - acc: 0.3558\n",
      "Epoch 59/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4680 - acc: 0.3598\n",
      "Epoch 60/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4655 - acc: 0.3615\n",
      "Epoch 61/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4634 - acc: 0.3617\n",
      "Epoch 62/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4609 - acc: 0.3618\n",
      "Epoch 63/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4601 - acc: 0.3603\n",
      "Epoch 64/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4579 - acc: 0.3640\n",
      "Epoch 65/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4553 - acc: 0.3631\n",
      "Epoch 66/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4544 - acc: 0.3640\n",
      "Epoch 67/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4517 - acc: 0.3648\n",
      "Epoch 68/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4499 - acc: 0.3676\n",
      "Epoch 69/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4483 - acc: 0.3669\n",
      "Epoch 70/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4478 - acc: 0.3656\n",
      "Epoch 71/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4458 - acc: 0.3640\n",
      "Epoch 72/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4440 - acc: 0.3690\n",
      "Epoch 73/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4413 - acc: 0.3698\n",
      "Epoch 74/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4406 - acc: 0.3701\n",
      "Epoch 75/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4389 - acc: 0.3674\n",
      "Epoch 76/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4385 - acc: 0.3692\n",
      "Epoch 77/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4365 - acc: 0.3678\n",
      "Epoch 78/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4356 - acc: 0.3707\n",
      "Epoch 79/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4342 - acc: 0.3688\n",
      "Epoch 80/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4322 - acc: 0.3714\n",
      "Epoch 81/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4310 - acc: 0.3705\n",
      "Epoch 82/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4299 - acc: 0.3710\n",
      "Epoch 83/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4290 - acc: 0.3724\n",
      "Epoch 84/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4264 - acc: 0.3760\n",
      "Epoch 85/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4259 - acc: 0.3744\n",
      "Epoch 86/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4256 - acc: 0.3734\n",
      "Epoch 87/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4250 - acc: 0.3733\n",
      "Epoch 88/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4236 - acc: 0.3739\n",
      "Epoch 89/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4217 - acc: 0.3745\n",
      "Epoch 90/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4202 - acc: 0.3773\n",
      "Epoch 91/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4195 - acc: 0.3756\n",
      "Epoch 92/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4192 - acc: 0.3752\n",
      "Epoch 93/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4179 - acc: 0.3746\n",
      "Epoch 94/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4174 - acc: 0.3759\n",
      "Epoch 95/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4159 - acc: 0.3757\n",
      "Epoch 96/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4142 - acc: 0.3761\n",
      "Epoch 97/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.4126 - acc: 0.3801\n",
      "Epoch 98/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4127 - acc: 0.3796\n",
      "Epoch 99/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4122 - acc: 0.3788\n",
      "Epoch 100/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.4116 - acc: 0.3759\n",
      "Epoch 1/100\n",
      "26329/26329 [==============================] - 1s 27us/step - loss: 2.3750 - acc: 0.1486\n",
      "Epoch 2/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 2.0791 - acc: 0.2444\n",
      "Epoch 3/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 1.8001 - acc: 0.3183\n",
      "Epoch 4/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.5865 - acc: 0.3792\n",
      "Epoch 5/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.3927 - acc: 0.4234\n",
      "Epoch 6/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.2330 - acc: 0.4800\n",
      "Epoch 7/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.1103 - acc: 0.5775\n",
      "Epoch 8/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 1.0119 - acc: 0.6225\n",
      "Epoch 9/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.9303 - acc: 0.6453\n",
      "Epoch 10/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.8622 - acc: 0.6524\n",
      "Epoch 11/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.8093 - acc: 0.6625\n",
      "Epoch 12/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.7673 - acc: 0.6636\n",
      "Epoch 13/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.7296 - acc: 0.6785\n",
      "Epoch 14/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.6949 - acc: 0.6891\n",
      "Epoch 15/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.6575 - acc: 0.7009\n",
      "Epoch 16/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.6111 - acc: 0.7231\n",
      "Epoch 17/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.5644 - acc: 0.7548\n",
      "Epoch 18/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.5175 - acc: 0.7875\n",
      "Epoch 19/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.4722 - acc: 0.8065\n",
      "Epoch 20/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.4399 - acc: 0.8200\n",
      "Epoch 21/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.4196 - acc: 0.8241\n",
      "Epoch 22/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.4060 - acc: 0.8249\n",
      "Epoch 23/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.3942 - acc: 0.8297\n",
      "Epoch 24/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3864 - acc: 0.8307\n",
      "Epoch 25/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3787 - acc: 0.8346\n",
      "Epoch 26/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3734 - acc: 0.8338\n",
      "Epoch 27/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3686 - acc: 0.8347\n",
      "Epoch 28/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3636 - acc: 0.8346\n",
      "Epoch 29/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3583 - acc: 0.8382\n",
      "Epoch 30/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3554 - acc: 0.8351\n",
      "Epoch 31/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3507 - acc: 0.8389\n",
      "Epoch 32/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3482 - acc: 0.8397\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3453 - acc: 0.8404\n",
      "Epoch 34/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3432 - acc: 0.8428\n",
      "Epoch 35/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3390 - acc: 0.8433\n",
      "Epoch 36/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3373 - acc: 0.8421\n",
      "Epoch 37/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3364 - acc: 0.8420\n",
      "Epoch 38/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.3332 - acc: 0.8460\n",
      "Epoch 39/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3304 - acc: 0.8452\n",
      "Epoch 40/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3286 - acc: 0.8466\n",
      "Epoch 41/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3273 - acc: 0.8459\n",
      "Epoch 42/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3254 - acc: 0.8450\n",
      "Epoch 43/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3227 - acc: 0.8470\n",
      "Epoch 44/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3213 - acc: 0.8504\n",
      "Epoch 45/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3189 - acc: 0.8509\n",
      "Epoch 46/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3168 - acc: 0.8513\n",
      "Epoch 47/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3154 - acc: 0.8516\n",
      "Epoch 48/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3144 - acc: 0.8566\n",
      "Epoch 49/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3118 - acc: 0.8544\n",
      "Epoch 50/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3104 - acc: 0.8542\n",
      "Epoch 51/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3082 - acc: 0.8551\n",
      "Epoch 52/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3076 - acc: 0.8567\n",
      "Epoch 53/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3060 - acc: 0.8591\n",
      "Epoch 54/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3040 - acc: 0.8577\n",
      "Epoch 55/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.3027 - acc: 0.8587\n",
      "Epoch 56/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3013 - acc: 0.8603\n",
      "Epoch 57/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.3010 - acc: 0.8578\n",
      "Epoch 58/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2995 - acc: 0.8621\n",
      "Epoch 59/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2981 - acc: 0.8638\n",
      "Epoch 60/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.2971 - acc: 0.8624\n",
      "Epoch 61/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2957 - acc: 0.8610\n",
      "Epoch 62/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2929 - acc: 0.8626\n",
      "Epoch 63/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2930 - acc: 0.8639\n",
      "Epoch 64/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2906 - acc: 0.8650\n",
      "Epoch 65/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2889 - acc: 0.8647\n",
      "Epoch 66/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2888 - acc: 0.8632\n",
      "Epoch 67/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2887 - acc: 0.8630\n",
      "Epoch 68/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2862 - acc: 0.8638\n",
      "Epoch 69/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2856 - acc: 0.8653\n",
      "Epoch 70/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2829 - acc: 0.8666\n",
      "Epoch 71/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.2826 - acc: 0.8671\n",
      "Epoch 72/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2807 - acc: 0.8663\n",
      "Epoch 73/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2782 - acc: 0.8692\n",
      "Epoch 74/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2787 - acc: 0.8681\n",
      "Epoch 75/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2755 - acc: 0.8708\n",
      "Epoch 76/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2737 - acc: 0.8710\n",
      "Epoch 77/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.2728 - acc: 0.8710\n",
      "Epoch 78/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2717 - acc: 0.8731\n",
      "Epoch 79/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2705 - acc: 0.8739\n",
      "Epoch 80/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2695 - acc: 0.8754\n",
      "Epoch 81/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2681 - acc: 0.8760\n",
      "Epoch 82/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2630 - acc: 0.8860\n",
      "Epoch 83/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2575 - acc: 0.8937\n",
      "Epoch 84/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2555 - acc: 0.8968\n",
      "Epoch 85/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2533 - acc: 0.8994\n",
      "Epoch 86/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2507 - acc: 0.9012\n",
      "Epoch 87/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.2489 - acc: 0.9025\n",
      "Epoch 88/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.2465 - acc: 0.9040\n",
      "Epoch 89/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2457 - acc: 0.9072\n",
      "Epoch 90/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2452 - acc: 0.9074\n",
      "Epoch 91/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2417 - acc: 0.9077\n",
      "Epoch 92/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2414 - acc: 0.9090\n",
      "Epoch 93/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2416 - acc: 0.9084\n",
      "Epoch 94/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2398 - acc: 0.9114\n",
      "Epoch 95/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2400 - acc: 0.9104\n",
      "Epoch 96/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2380 - acc: 0.9101\n",
      "Epoch 97/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2371 - acc: 0.9120\n",
      "Epoch 98/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2368 - acc: 0.9114\n",
      "Epoch 99/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2365 - acc: 0.9115\n",
      "Epoch 100/100\n",
      "26329/26329 [==============================] - 0s 14us/step - loss: 0.2381 - acc: 0.9123\n",
      "Epoch 1/100\n",
      "26329/26329 [==============================] - 1s 35us/step - loss: 2.3973 - acc: 0.1015\n",
      "Epoch 2/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 2.3969 - acc: 0.1085\n",
      "Epoch 3/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 2.3962 - acc: 0.1254\n",
      "Epoch 4/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 2.3952 - acc: 0.1285\n",
      "Epoch 5/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 2.3930 - acc: 0.1029\n",
      "Epoch 6/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 2.3871 - acc: 0.1196\n",
      "Epoch 7/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 2.3630 - acc: 0.1359\n",
      "Epoch 8/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 2.2681 - acc: 0.1597\n",
      "Epoch 9/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 2.1452 - acc: 0.1869\n",
      "Epoch 10/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 2.0136 - acc: 0.2014\n",
      "Epoch 11/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.8712 - acc: 0.2144\n",
      "Epoch 12/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.7865 - acc: 0.2275\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.7330 - acc: 0.2459\n",
      "Epoch 14/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.6953 - acc: 0.2674\n",
      "Epoch 15/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.6681 - acc: 0.2855\n",
      "Epoch 16/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.6464 - acc: 0.2935\n",
      "Epoch 17/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.6280 - acc: 0.3065\n",
      "Epoch 18/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.6128 - acc: 0.3120\n",
      "Epoch 19/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.6000 - acc: 0.3148\n",
      "Epoch 20/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5893 - acc: 0.3195\n",
      "Epoch 21/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.5789 - acc: 0.3230\n",
      "Epoch 22/100\n",
      "26329/26329 [==============================] - ETA: 0s - loss: 1.5695 - acc: 0.324 - 0s 17us/step - loss: 1.5695 - acc: 0.3256\n",
      "Epoch 23/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5626 - acc: 0.3289: 0s - loss: 1.5645 - acc: \n",
      "Epoch 24/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5523 - acc: 0.3346\n",
      "Epoch 25/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5453 - acc: 0.3354\n",
      "Epoch 26/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5373 - acc: 0.3385\n",
      "Epoch 27/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5319 - acc: 0.3397\n",
      "Epoch 28/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5262 - acc: 0.3389\n",
      "Epoch 29/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5195 - acc: 0.3428\n",
      "Epoch 30/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5137 - acc: 0.3447\n",
      "Epoch 31/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.5085 - acc: 0.3473\n",
      "Epoch 32/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.5052 - acc: 0.3486\n",
      "Epoch 33/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4989 - acc: 0.3504\n",
      "Epoch 34/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4936 - acc: 0.3522\n",
      "Epoch 35/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4906 - acc: 0.3519\n",
      "Epoch 36/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4828 - acc: 0.3588\n",
      "Epoch 37/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4789 - acc: 0.3606\n",
      "Epoch 38/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4746 - acc: 0.3624\n",
      "Epoch 39/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4691 - acc: 0.3627\n",
      "Epoch 40/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4665 - acc: 0.3640\n",
      "Epoch 41/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4609 - acc: 0.3672\n",
      "Epoch 42/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4564 - acc: 0.3670\n",
      "Epoch 43/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4504 - acc: 0.3724\n",
      "Epoch 44/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4463 - acc: 0.3749\n",
      "Epoch 45/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.4409 - acc: 0.3747\n",
      "Epoch 46/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4399 - acc: 0.3748\n",
      "Epoch 47/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4342 - acc: 0.3746\n",
      "Epoch 48/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4309 - acc: 0.3783\n",
      "Epoch 49/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4277 - acc: 0.3794\n",
      "Epoch 50/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4241 - acc: 0.3820\n",
      "Epoch 51/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4197 - acc: 0.3884\n",
      "Epoch 52/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4180 - acc: 0.3908\n",
      "Epoch 53/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4130 - acc: 0.3934\n",
      "Epoch 54/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4137 - acc: 0.3932\n",
      "Epoch 55/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4105 - acc: 0.3970\n",
      "Epoch 56/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4062 - acc: 0.4032\n",
      "Epoch 57/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4099 - acc: 0.3953\n",
      "Epoch 58/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4071 - acc: 0.4009\n",
      "Epoch 59/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.4003 - acc: 0.4096\n",
      "Epoch 60/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.3981 - acc: 0.4129\n",
      "Epoch 61/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3971 - acc: 0.4137\n",
      "Epoch 62/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3953 - acc: 0.4161\n",
      "Epoch 63/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3887 - acc: 0.4208\n",
      "Epoch 64/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3897 - acc: 0.4180\n",
      "Epoch 65/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3878 - acc: 0.4195\n",
      "Epoch 66/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3824 - acc: 0.4218\n",
      "Epoch 67/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3879 - acc: 0.4212\n",
      "Epoch 68/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3833 - acc: 0.4262\n",
      "Epoch 69/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3747 - acc: 0.4249\n",
      "Epoch 70/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.3764 - acc: 0.4327\n",
      "Epoch 71/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3704 - acc: 0.4361\n",
      "Epoch 72/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3675 - acc: 0.4395\n",
      "Epoch 73/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3679 - acc: 0.4442\n",
      "Epoch 74/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3578 - acc: 0.4492\n",
      "Epoch 75/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.3538 - acc: 0.4492\n",
      "Epoch 76/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3559 - acc: 0.4521\n",
      "Epoch 77/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3429 - acc: 0.4540\n",
      "Epoch 78/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.3414 - acc: 0.4557\n",
      "Epoch 79/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.3383 - acc: 0.4589\n",
      "Epoch 80/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3292 - acc: 0.4653\n",
      "Epoch 81/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.3175 - acc: 0.4738\n",
      "Epoch 82/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.2880 - acc: 0.4857\n",
      "Epoch 83/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.2641 - acc: 0.4856\n",
      "Epoch 84/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.2536 - acc: 0.4825\n",
      "Epoch 85/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.2389 - acc: 0.4830\n",
      "Epoch 86/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.2322 - acc: 0.4882\n",
      "Epoch 87/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.2228 - acc: 0.4945\n",
      "Epoch 88/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.2116 - acc: 0.5043\n",
      "Epoch 89/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.2106 - acc: 0.5141\n",
      "Epoch 90/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.2079 - acc: 0.5100\n",
      "Epoch 91/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.2026 - acc: 0.5167\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.1869 - acc: 0.5160\n",
      "Epoch 93/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.1911 - acc: 0.5175\n",
      "Epoch 94/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.1821 - acc: 0.5143\n",
      "Epoch 95/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.1838 - acc: 0.5192\n",
      "Epoch 96/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.1840 - acc: 0.5111\n",
      "Epoch 97/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.1753 - acc: 0.5205\n",
      "Epoch 98/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.1745 - acc: 0.5108\n",
      "Epoch 99/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.1718 - acc: 0.5131\n",
      "Epoch 100/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 1.1682 - acc: 0.5147\n",
      "Epoch 1/100\n",
      "26329/26329 [==============================] - 1s 37us/step - loss: 2.3279 - acc: 0.1233\n",
      "Epoch 2/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 2.1757 - acc: 0.1863\n",
      "Epoch 3/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 2.1157 - acc: 0.2044\n",
      "Epoch 4/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.9861 - acc: 0.2463\n",
      "Epoch 5/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 1.7390 - acc: 0.3002\n",
      "Epoch 6/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 1.5456 - acc: 0.3283\n",
      "Epoch 7/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.4592 - acc: 0.3403\n",
      "Epoch 8/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.4073 - acc: 0.3625\n",
      "Epoch 9/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 1.3642 - acc: 0.3751\n",
      "Epoch 10/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 1.3184 - acc: 0.4002\n",
      "Epoch 11/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.2652 - acc: 0.4177\n",
      "Epoch 12/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.2010 - acc: 0.4334\n",
      "Epoch 13/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.1046 - acc: 0.4753\n",
      "Epoch 14/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 1.0315 - acc: 0.5210\n",
      "Epoch 15/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.9855 - acc: 0.5489\n",
      "Epoch 16/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.9538 - acc: 0.5770\n",
      "Epoch 17/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.9163 - acc: 0.6124\n",
      "Epoch 18/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.8873 - acc: 0.6284\n",
      "Epoch 19/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.8632 - acc: 0.6394\n",
      "Epoch 20/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.8480 - acc: 0.6446\n",
      "Epoch 21/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.8403 - acc: 0.6473\n",
      "Epoch 22/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.8269 - acc: 0.6524\n",
      "Epoch 23/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.8154 - acc: 0.6533\n",
      "Epoch 24/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.8028 - acc: 0.6587\n",
      "Epoch 25/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7908 - acc: 0.6633\n",
      "Epoch 26/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7869 - acc: 0.6645\n",
      "Epoch 27/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7749 - acc: 0.6681\n",
      "Epoch 28/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7678 - acc: 0.6713\n",
      "Epoch 29/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.7618 - acc: 0.6729\n",
      "Epoch 30/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7509 - acc: 0.6757\n",
      "Epoch 31/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7490 - acc: 0.6772\n",
      "Epoch 32/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7392 - acc: 0.6818\n",
      "Epoch 33/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7371 - acc: 0.6827\n",
      "Epoch 34/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7266 - acc: 0.6826\n",
      "Epoch 35/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7136 - acc: 0.6903\n",
      "Epoch 36/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.7158 - acc: 0.6874\n",
      "Epoch 37/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.6921 - acc: 0.6938\n",
      "Epoch 38/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.6668 - acc: 0.7105\n",
      "Epoch 39/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.6146 - acc: 0.7365\n",
      "Epoch 40/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.5871 - acc: 0.7534\n",
      "Epoch 41/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.5603 - acc: 0.7643\n",
      "Epoch 42/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.5254 - acc: 0.7788\n",
      "Epoch 43/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.5183 - acc: 0.7789\n",
      "Epoch 44/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.5052 - acc: 0.7860\n",
      "Epoch 45/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.4865 - acc: 0.7889\n",
      "Epoch 46/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.4922 - acc: 0.7864\n",
      "Epoch 47/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.4806 - acc: 0.7895\n",
      "Epoch 48/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.4591 - acc: 0.7975\n",
      "Epoch 49/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.4409 - acc: 0.8124\n",
      "Epoch 50/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.3952 - acc: 0.8271\n",
      "Epoch 51/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.3824 - acc: 0.8417\n",
      "Epoch 52/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.4184 - acc: 0.8419\n",
      "Epoch 53/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.3390 - acc: 0.8702\n",
      "Epoch 54/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.3126 - acc: 0.8769\n",
      "Epoch 55/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.3285 - acc: 0.8777\n",
      "Epoch 56/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.2907 - acc: 0.8880\n",
      "Epoch 57/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.3484 - acc: 0.8798\n",
      "Epoch 58/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2766 - acc: 0.8912\n",
      "Epoch 59/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2732 - acc: 0.8942\n",
      "Epoch 60/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2699 - acc: 0.8970\n",
      "Epoch 61/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2678 - acc: 0.8992\n",
      "Epoch 62/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2639 - acc: 0.8995\n",
      "Epoch 63/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.2543 - acc: 0.9056\n",
      "Epoch 64/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2560 - acc: 0.9033\n",
      "Epoch 65/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.2490 - acc: 0.9049\n",
      "Epoch 66/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2607 - acc: 0.9003\n",
      "Epoch 67/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2536 - acc: 0.9039\n",
      "Epoch 68/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2564 - acc: 0.9054\n",
      "Epoch 69/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.2517 - acc: 0.9081\n",
      "Epoch 70/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.2507 - acc: 0.9070\n",
      "Epoch 71/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2408 - acc: 0.9094\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2380 - acc: 0.9133\n",
      "Epoch 73/100\n",
      "26329/26329 [==============================] - ETA: 0s - loss: 0.2456 - acc: 0.908 - 0s 17us/step - loss: 0.2461 - acc: 0.9090\n",
      "Epoch 74/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2434 - acc: 0.9093\n",
      "Epoch 75/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2415 - acc: 0.9098\n",
      "Epoch 76/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2418 - acc: 0.9111\n",
      "Epoch 77/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2437 - acc: 0.9107\n",
      "Epoch 78/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.2408 - acc: 0.9093\n",
      "Epoch 79/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.2475 - acc: 0.9090\n",
      "Epoch 80/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.2399 - acc: 0.9128\n",
      "Epoch 81/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.2460 - acc: 0.9093\n",
      "Epoch 82/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.2329 - acc: 0.9125\n",
      "Epoch 83/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2385 - acc: 0.9131\n",
      "Epoch 84/100\n",
      "26329/26329 [==============================] - 1s 23us/step - loss: 0.2295 - acc: 0.9158\n",
      "Epoch 85/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2308 - acc: 0.9156\n",
      "Epoch 86/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.2279 - acc: 0.9172\n",
      "Epoch 87/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.2373 - acc: 0.9117\n",
      "Epoch 88/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.2306 - acc: 0.9131\n",
      "Epoch 89/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2279 - acc: 0.9169\n",
      "Epoch 90/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2334 - acc: 0.9151\n",
      "Epoch 91/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2314 - acc: 0.9158\n",
      "Epoch 92/100\n",
      "26329/26329 [==============================] - 1s 22us/step - loss: 0.2247 - acc: 0.9171\n",
      "Epoch 93/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2227 - acc: 0.9186\n",
      "Epoch 94/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2262 - acc: 0.9177\n",
      "Epoch 95/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2199 - acc: 0.9202\n",
      "Epoch 96/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2309 - acc: 0.9182\n",
      "Epoch 97/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.2261 - acc: 0.9171\n",
      "Epoch 98/100\n",
      "26329/26329 [==============================] - ETA: 0s - loss: 0.2283 - acc: 0.916 - 0s 17us/step - loss: 0.2290 - acc: 0.9168\n",
      "Epoch 99/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.2221 - acc: 0.9208\n",
      "Epoch 100/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2308 - acc: 0.9178\n",
      "Epoch 1/100\n",
      "26329/26329 [==============================] - 1s 50us/step - loss: 2.3979 - acc: 0.0915\n",
      "Epoch 2/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3979 - acc: 0.0926\n",
      "Epoch 3/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3979 - acc: 0.0924\n",
      "Epoch 4/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3979 - acc: 0.0921\n",
      "Epoch 5/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 6/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 7/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 2.3978 - acc: 0.0927\n",
      "Epoch 8/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 9/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 10/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 11/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 12/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 13/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 14/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0924\n",
      "Epoch 15/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 16/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 17/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0926\n",
      "Epoch 18/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 19/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 20/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 21/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 22/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 23/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 24/100\n",
      "26329/26329 [==============================] - 0s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 25/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 26/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 27/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 28/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0928\n",
      "Epoch 29/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 30/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 31/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 2.3978 - acc: 0.0923\n",
      "Epoch 32/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 33/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 34/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 35/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 36/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 37/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 2.3978 - acc: 0.0931\n",
      "Epoch 38/100\n",
      " 2560/26329 [=>............................] - ETA: 0s - loss: 2.3977 - acc: 0.0867"
     ]
    }
   ],
   "source": [
    "nnpt.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnpt.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnpt.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot Matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
