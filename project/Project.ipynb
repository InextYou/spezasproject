{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"../data/Component_Faults_Data.csv\"\n",
    "df = pd.read_csv(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, :48].values\n",
    "y = df[\"class\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newer Solution for testing NN-Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "    \n",
    "class NeuralNetworkParameterTester:\n",
    "\n",
    "    # params for building model\n",
    "    config = {\n",
    "        'input_layer_dim': 48,\n",
    "        'output_layer_dim': 11,\n",
    "        'number_of_hidden_layers': 1,\n",
    "        'number_of_units_per_layer': 10,\n",
    "        'epochs': 100,\n",
    "        'batch_size': 64,\n",
    "        'activation_function': 'relu',\n",
    "        'loss_function': 'mean_squared_error',\n",
    "        'optimizer': 'sgd'}\n",
    "\n",
    "    \n",
    "    test_param_name = None  # Name of the div parameter\n",
    "    test_param_val = None  # Name of the div parameter\n",
    "    \n",
    "    result = []  # Result of the test\n",
    "    \n",
    "    # schema of result\n",
    "    \n",
    "\n",
    "    # [\n",
    "    #    [\n",
    "    #        'test_param_val': 1,\n",
    "    #        'params': [\n",
    "    #            'input_layer_dim': 48,\n",
    "    #            'output_layer_dim': 11,\n",
    "    #            'activation_function': 'relu',\n",
    "    #            'loss_function': 'mean_squared_error',\n",
    "    #            'optimizer': 'sgd',\n",
    "    #            'number_of_hidden_layers': 1,\n",
    "    #            'number_of_units_per_layer': 10]\n",
    "    #        'result': [\n",
    "    #            'model': model,\n",
    "    #            'acurracy': 0.923\n",
    "    #        ]\n",
    "    #    ],\n",
    "    #    [\n",
    "    #        'test_param_val': 2,\n",
    "    #        'params': [\n",
    "    #            'input_layer_dim': 48,\n",
    "    #            'output_layer_dim': 11,\n",
    "    #            'activation_function': 'relu',\n",
    "    #            'loss_function': 'mean_squared_error',\n",
    "    #            'optimizer': 'sgd',\n",
    "    #            'number_of_hidden_layers': 2,\n",
    "    #            'number_of_units_per_layer': 10]\n",
    "    #        'result': [\n",
    "    #            'model': model,\n",
    "    #            'acurracy': 0.923\n",
    "    #        ]\n",
    "    #    ],\n",
    "    #   [...........]\n",
    "    # ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, test_param_name, test_param_val):\n",
    "        \n",
    "        # 'param_name' and 'param_val' must be of right type and not None\n",
    "        # if this is the case, they will be set\n",
    "        \n",
    "        if test_param_name is not None and isinstance(test_param_name, str):\n",
    "            self.test_param_name = test_param_name\n",
    "            if test_param_val is not None and (isinstance(test_param_val, int) or isinstance(test_param_val, list)):\n",
    "                self.test_param_val = test_param_val\n",
    "                self.config[self.test_param_name] = self.test_param_val\n",
    "            else:\n",
    "                print(\"'test_param_val' must be of type int or list\")\n",
    "                sys.exit(0)\n",
    "        else:\n",
    "            print(\"'param_name' must be of type str\")\n",
    "            sys.exit(0)\n",
    "        #self.result.append({\"test_param_name\": self.test_param_name})\n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        for val in self.config[self.test_param_name]:\n",
    "            \n",
    "            print(val)\n",
    "            \n",
    "            # prepare config for individual test\n",
    "            test_config = self.config  \n",
    "            test_config[self.test_param_name] = val\n",
    "        \n",
    "            model = self.__build(test_config)\n",
    "            trained_model, history = self.__train(test_config, model)\n",
    "            accuracy = self.__test(trained_model)\n",
    "            \n",
    "            # save result\n",
    "            # TODO tes param gets overritwn\n",
    "            self.result.append({\"test_param_val\": val, \n",
    "                                'params': test_config.copy(), \n",
    "                                \"result\": {'model': trained_model, \n",
    "                                           'accuracy': accuracy}})\n",
    "\n",
    "\n",
    "    def __build(self, test_config):\n",
    "\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        \n",
    "        # Input layer\n",
    "        # TODO output here\n",
    "        model.add(Dense(test_config['number_of_units_per_layer'], \n",
    "                        input_dim=test_config['input_layer_dim'], \n",
    "                        activation=test_config['activation_function']))\n",
    "        # Hidden layer\n",
    "        for i in range(test_config['number_of_hidden_layers']):\n",
    "            model.add(Dense(test_config['number_of_units_per_layer'], \n",
    "                            activation=test_config['activation_function']))\n",
    "        # Output layer\n",
    "        model.add(Dense(test_config['output_layer_dim'], \n",
    "                        activation=\"softmax\"))\n",
    "\n",
    "        model.compile(loss=test_config['loss_function'], \n",
    "                      optimizer=test_config['optimizer'], \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def __train(self, test_config, model):\n",
    "        # self.param_list.update({\"epochs\": epochs, \"batch_size\": batch_size})\n",
    "                                    \n",
    "        # xtrain and ytrain are from preprocessing\n",
    "        history = model.fit(x_train, \n",
    "                            y_train, \n",
    "                            epochs=test_config['epochs'], \n",
    "                            batch_size=test_config['batch_size'])\n",
    "                                    \n",
    "        return model, history\n",
    "    \n",
    "    @staticmethod\n",
    "    def __test(model):\n",
    "        y_pred = model.predict(x_test)\n",
    "        # Converting predictions to label\n",
    "        pred = list()\n",
    "        for i in range(len(y_pred)):\n",
    "            pred.append(np.argmax(y_pred[i]))\n",
    "        # Converting one hot encoded test label to label\n",
    "        test = list()\n",
    "        for i in range(len(y_test)):\n",
    "            test.append(np.argmax(y_test[i]))\n",
    "\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score(pred, test)\n",
    "        return accuracy\n",
    "                                    \n",
    "                                    \n",
    "    # Setter methods for setting single parameters\n",
    "    \n",
    "    def set_number_of_hidden_layers(self, val):\n",
    "        self.config[\"number_of_hidden_layers\"] = val\n",
    "        \n",
    "    def set_number_of_units_per_layer(self, val):\n",
    "        self.config[\"number_of_units_per_layer\"] = val\n",
    "        \n",
    "    def set_activation_function(self, val):\n",
    "        self.config[\"activation_function\"] = val\n",
    "        \n",
    "    def set_epochs(self, val):\n",
    "        self.config[\"epochs\"] = val\n",
    "        \n",
    "    def set_batch_size(self, val):\n",
    "        self.config[\"batch_size\"] = val\n",
    "        \n",
    "    def set_loss_function(self, val):\n",
    "        self.config[\"loss_function\"] = val\n",
    "        \n",
    "    def set_optimizer(self, val):\n",
    "        self.config[\"optimizer\"] = val\n",
    "        \n",
    "    def reset_config(self):\n",
    "        config = {\n",
    "            'input_layer_dim': 48,\n",
    "            'output_layer_dim': 11,\n",
    "            'number_of_hidden_layers': 1,\n",
    "            'number_of_units_per_layer': 10,\n",
    "            'epochs': 100,\n",
    "            'batch_size': 64,\n",
    "            'activation_function': 'relu',\n",
    "            'loss_function': 'mean_squared_error',\n",
    "            'optimizer': 'sgd'}\n",
    "        \n",
    "        config[self.test_param_name] = self.test_param_val\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "    # Plot result\n",
    "    \n",
    "    def plot_result(self, dim=2):\n",
    "        if dim == 2:\n",
    "            self.__plot_2d()\n",
    "\n",
    "    def __plot_2d(self):\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        test_param_val = [element['test_param_val'] for element in self.result]\n",
    "        accuracy_result = [element['result']['accuracy'] for element in self.result]\n",
    "        \n",
    "        print(test_param_val)\n",
    "        print(accuracy_result)\n",
    "\n",
    "        plt.plot(test_param_val, accuracy_result)\n",
    "        plt.title(\"Accuracy per \" + \"'\" + self.test_param_name + \"'\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(self.test_param_name)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NN with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "26329/26329 [==============================] - 1s 37us/step - loss: 0.0833 - acc: 0.0910\n",
      "Epoch 2/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0828 - acc: 0.1071\n",
      "Epoch 3/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0822 - acc: 0.1391\n",
      "Epoch 4/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0816 - acc: 0.1648\n",
      "Epoch 5/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0808 - acc: 0.1744\n",
      "Epoch 6/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0799 - acc: 0.1776\n",
      "Epoch 7/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0788 - acc: 0.1804\n",
      "Epoch 8/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0778 - acc: 0.1853\n",
      "Epoch 9/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0770 - acc: 0.1915\n",
      "Epoch 10/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0762 - acc: 0.1959\n",
      "Epoch 11/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0756 - acc: 0.1981\n",
      "Epoch 12/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0750 - acc: 0.2005\n",
      "Epoch 13/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0745 - acc: 0.2027\n",
      "Epoch 14/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0740 - acc: 0.2034\n",
      "Epoch 15/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0735 - acc: 0.2044\n",
      "Epoch 16/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0731 - acc: 0.2059\n",
      "Epoch 17/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0727 - acc: 0.2084\n",
      "Epoch 18/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0723 - acc: 0.2104\n",
      "Epoch 19/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0719 - acc: 0.2141\n",
      "Epoch 20/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0715 - acc: 0.2179\n",
      "Epoch 21/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0711 - acc: 0.2231\n",
      "Epoch 22/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0708 - acc: 0.2276: 0s - loss: 0.0711 - acc:\n",
      "Epoch 23/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0704 - acc: 0.2321\n",
      "Epoch 24/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0701 - acc: 0.2391\n",
      "Epoch 25/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0698 - acc: 0.2449\n",
      "Epoch 26/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0694 - acc: 0.2514\n",
      "Epoch 27/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0691 - acc: 0.2584\n",
      "Epoch 28/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0688 - acc: 0.2650\n",
      "Epoch 29/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0685 - acc: 0.2721\n",
      "Epoch 30/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0682 - acc: 0.2790\n",
      "Epoch 31/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0680 - acc: 0.2861\n",
      "Epoch 32/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0677 - acc: 0.2909\n",
      "Epoch 33/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0674 - acc: 0.2979\n",
      "Epoch 34/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0671 - acc: 0.3047\n",
      "Epoch 35/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0668 - acc: 0.3114\n",
      "Epoch 36/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0666 - acc: 0.3174\n",
      "Epoch 37/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0663 - acc: 0.3238\n",
      "Epoch 38/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0660 - acc: 0.3309\n",
      "Epoch 39/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0657 - acc: 0.3359\n",
      "Epoch 40/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0655 - acc: 0.3406\n",
      "Epoch 41/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0652 - acc: 0.3476\n",
      "Epoch 42/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0649 - acc: 0.3524\n",
      "Epoch 43/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0646 - acc: 0.3579\n",
      "Epoch 44/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0643 - acc: 0.3634\n",
      "Epoch 45/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0641 - acc: 0.3688\n",
      "Epoch 46/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0638 - acc: 0.3746\n",
      "Epoch 47/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0635 - acc: 0.3809\n",
      "Epoch 48/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0632 - acc: 0.3855\n",
      "Epoch 49/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0629 - acc: 0.3908\n",
      "Epoch 50/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0626 - acc: 0.3962\n",
      "Epoch 51/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0623 - acc: 0.4022\n",
      "Epoch 52/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0620 - acc: 0.4073\n",
      "Epoch 53/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0617 - acc: 0.4131\n",
      "Epoch 54/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0614 - acc: 0.4199\n",
      "Epoch 55/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0611 - acc: 0.4256\n",
      "Epoch 56/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0608 - acc: 0.4325\n",
      "Epoch 57/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0605 - acc: 0.4392\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0602 - acc: 0.4446\n",
      "Epoch 59/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0599 - acc: 0.4499\n",
      "Epoch 60/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0595 - acc: 0.4553\n",
      "Epoch 61/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0592 - acc: 0.4612\n",
      "Epoch 62/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0589 - acc: 0.4667\n",
      "Epoch 63/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0586 - acc: 0.4733\n",
      "Epoch 64/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0582 - acc: 0.4787\n",
      "Epoch 65/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0579 - acc: 0.4833\n",
      "Epoch 66/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0576 - acc: 0.4877\n",
      "Epoch 67/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0572 - acc: 0.4926\n",
      "Epoch 68/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0569 - acc: 0.4968\n",
      "Epoch 69/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0566 - acc: 0.5013\n",
      "Epoch 70/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0562 - acc: 0.5070\n",
      "Epoch 71/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0559 - acc: 0.5117\n",
      "Epoch 72/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0555 - acc: 0.5168\n",
      "Epoch 73/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0552 - acc: 0.5213\n",
      "Epoch 74/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0548 - acc: 0.5263\n",
      "Epoch 75/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0545 - acc: 0.5315\n",
      "Epoch 76/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0541 - acc: 0.5370\n",
      "Epoch 77/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0538 - acc: 0.5413\n",
      "Epoch 78/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0534 - acc: 0.5465\n",
      "Epoch 79/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0531 - acc: 0.5516\n",
      "Epoch 80/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0527 - acc: 0.5566\n",
      "Epoch 81/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0524 - acc: 0.5611\n",
      "Epoch 82/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0520 - acc: 0.5667\n",
      "Epoch 83/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0517 - acc: 0.5721\n",
      "Epoch 84/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0513 - acc: 0.5772\n",
      "Epoch 85/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0509 - acc: 0.5824\n",
      "Epoch 86/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0506 - acc: 0.5884\n",
      "Epoch 87/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0502 - acc: 0.5938\n",
      "Epoch 88/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0498 - acc: 0.5996\n",
      "Epoch 89/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0494 - acc: 0.6056\n",
      "Epoch 90/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0490 - acc: 0.6106\n",
      "Epoch 91/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0487 - acc: 0.6156\n",
      "Epoch 92/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0483 - acc: 0.6202\n",
      "Epoch 93/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0479 - acc: 0.6268\n",
      "Epoch 94/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0474 - acc: 0.6316\n",
      "Epoch 95/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0470 - acc: 0.6378\n",
      "Epoch 96/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0466 - acc: 0.6435\n",
      "Epoch 97/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0462 - acc: 0.6475\n",
      "Epoch 98/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0457 - acc: 0.6529\n",
      "Epoch 99/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0453 - acc: 0.6576\n",
      "Epoch 100/100\n",
      "26329/26329 [==============================] - 0s 15us/step - loss: 0.0449 - acc: 0.6637\n",
      "4\n",
      "Epoch 1/100\n",
      "26329/26329 [==============================] - 1s 38us/step - loss: 0.0823 - acc: 0.0912\n",
      "Epoch 2/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0818 - acc: 0.0979\n",
      "Epoch 3/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0811 - acc: 0.0952\n",
      "Epoch 4/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0802 - acc: 0.0939\n",
      "Epoch 5/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0794 - acc: 0.0935\n",
      "Epoch 6/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0787 - acc: 0.0938\n",
      "Epoch 7/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0782 - acc: 0.0948\n",
      "Epoch 8/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0778 - acc: 0.0964\n",
      "Epoch 9/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0774 - acc: 0.0989\n",
      "Epoch 10/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0771 - acc: 0.1025\n",
      "Epoch 11/100\n",
      "26329/26329 [==============================] - 0s 19us/step - loss: 0.0768 - acc: 0.1070\n",
      "Epoch 12/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0766 - acc: 0.1116\n",
      "Epoch 13/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0763 - acc: 0.1184\n",
      "Epoch 14/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0761 - acc: 0.1250\n",
      "Epoch 15/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0759 - acc: 0.1335\n",
      "Epoch 16/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0758 - acc: 0.1423\n",
      "Epoch 17/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0756 - acc: 0.1513\n",
      "Epoch 18/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0755 - acc: 0.1591\n",
      "Epoch 19/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0753 - acc: 0.1677\n",
      "Epoch 20/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0752 - acc: 0.1781\n",
      "Epoch 21/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0750 - acc: 0.1866\n",
      "Epoch 22/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0749 - acc: 0.1950\n",
      "Epoch 23/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0748 - acc: 0.2050\n",
      "Epoch 24/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0747 - acc: 0.2137\n",
      "Epoch 25/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0746 - acc: 0.2230\n",
      "Epoch 26/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0745 - acc: 0.2310\n",
      "Epoch 27/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0743 - acc: 0.2389\n",
      "Epoch 28/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0742 - acc: 0.2461\n",
      "Epoch 29/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0741 - acc: 0.2529\n",
      "Epoch 30/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0740 - acc: 0.2606\n",
      "Epoch 31/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0739 - acc: 0.2686\n",
      "Epoch 32/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0738 - acc: 0.2738\n",
      "Epoch 33/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0736 - acc: 0.2802\n",
      "Epoch 34/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0735 - acc: 0.2860\n",
      "Epoch 35/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0734 - acc: 0.2906\n",
      "Epoch 36/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0732 - acc: 0.2956\n",
      "Epoch 37/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0731 - acc: 0.2997\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0730 - acc: 0.3042\n",
      "Epoch 39/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0728 - acc: 0.3091\n",
      "Epoch 40/100\n",
      "26329/26329 [==============================] - 0s 19us/step - loss: 0.0726 - acc: 0.3127\n",
      "Epoch 41/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0725 - acc: 0.3159\n",
      "Epoch 42/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0723 - acc: 0.3177\n",
      "Epoch 43/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0721 - acc: 0.3192\n",
      "Epoch 44/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0719 - acc: 0.3198\n",
      "Epoch 45/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0716 - acc: 0.3186\n",
      "Epoch 46/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0714 - acc: 0.3179\n",
      "Epoch 47/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0711 - acc: 0.3179\n",
      "Epoch 48/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0708 - acc: 0.3174\n",
      "Epoch 49/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0705 - acc: 0.3170\n",
      "Epoch 50/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0701 - acc: 0.3161\n",
      "Epoch 51/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0697 - acc: 0.3148\n",
      "Epoch 52/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0693 - acc: 0.3133\n",
      "Epoch 53/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0688 - acc: 0.3133\n",
      "Epoch 54/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0683 - acc: 0.3135\n",
      "Epoch 55/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0678 - acc: 0.3159\n",
      "Epoch 56/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0672 - acc: 0.3188\n",
      "Epoch 57/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0667 - acc: 0.3241\n",
      "Epoch 58/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0662 - acc: 0.3305\n",
      "Epoch 59/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0656 - acc: 0.3399\n",
      "Epoch 60/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0651 - acc: 0.3489\n",
      "Epoch 61/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0646 - acc: 0.3579\n",
      "Epoch 62/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0640 - acc: 0.3661\n",
      "Epoch 63/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0635 - acc: 0.3751\n",
      "Epoch 64/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0630 - acc: 0.3840\n",
      "Epoch 65/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0624 - acc: 0.3906\n",
      "Epoch 66/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0619 - acc: 0.4009\n",
      "Epoch 67/100\n",
      "26329/26329 [==============================] - 0s 19us/step - loss: 0.0614 - acc: 0.4081\n",
      "Epoch 68/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0609 - acc: 0.4160\n",
      "Epoch 69/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0604 - acc: 0.4241\n",
      "Epoch 70/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0598 - acc: 0.4329\n",
      "Epoch 71/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0593 - acc: 0.4401\n",
      "Epoch 72/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0588 - acc: 0.4495\n",
      "Epoch 73/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0583 - acc: 0.4572\n",
      "Epoch 74/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0578 - acc: 0.4680\n",
      "Epoch 75/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0573 - acc: 0.4755\n",
      "Epoch 76/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0568 - acc: 0.4846\n",
      "Epoch 77/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0563 - acc: 0.4928\n",
      "Epoch 78/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0558 - acc: 0.5008\n",
      "Epoch 79/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0553 - acc: 0.5093\n",
      "Epoch 80/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0549 - acc: 0.5173\n",
      "Epoch 81/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0544 - acc: 0.5261\n",
      "Epoch 82/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0539 - acc: 0.5355\n",
      "Epoch 83/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0534 - acc: 0.5433\n",
      "Epoch 84/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0529 - acc: 0.5506\n",
      "Epoch 85/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0525 - acc: 0.5572\n",
      "Epoch 86/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0520 - acc: 0.5657\n",
      "Epoch 87/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0515 - acc: 0.5732\n",
      "Epoch 88/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0511 - acc: 0.5806\n",
      "Epoch 89/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0506 - acc: 0.5885\n",
      "Epoch 90/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0501 - acc: 0.5933\n",
      "Epoch 91/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0496 - acc: 0.6015\n",
      "Epoch 92/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0491 - acc: 0.6081\n",
      "Epoch 93/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0486 - acc: 0.6156\n",
      "Epoch 94/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0481 - acc: 0.6217\n",
      "Epoch 95/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0476 - acc: 0.6285\n",
      "Epoch 96/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0471 - acc: 0.6337\n",
      "Epoch 97/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0466 - acc: 0.6425\n",
      "Epoch 98/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0460 - acc: 0.6496\n",
      "Epoch 99/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0454 - acc: 0.6561\n",
      "Epoch 100/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0448 - acc: 0.6653\n"
     ]
    }
   ],
   "source": [
    "nnpt = NeuralNetworkParameterTester(test_param_name=\"number_of_hidden_layers\", test_param_val=[2, 4])\n",
    "nnpt.set_number_of_units_per_layer(48)\n",
    "nnpt.set_epochs(100)\n",
    "nnpt.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_layer_dim': 48,\n",
       " 'output_layer_dim': 11,\n",
       " 'number_of_hidden_layers': 4,\n",
       " 'number_of_units_per_layer': 48,\n",
       " 'epochs': 100,\n",
       " 'batch_size': 64,\n",
       " 'activation_function': 'relu',\n",
       " 'loss_function': 'mean_squared_error',\n",
       " 'optimizer': 'sgd'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO sis isch falsch\n",
    "nnpt.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'test_param_val': 2,\n",
       "  'params': {'input_layer_dim': 48,\n",
       "   'output_layer_dim': 11,\n",
       "   'number_of_hidden_layers': 2,\n",
       "   'number_of_units_per_layer': 48,\n",
       "   'epochs': 100,\n",
       "   'batch_size': 64,\n",
       "   'activation_function': 'relu',\n",
       "   'loss_function': 'mean_squared_error',\n",
       "   'optimizer': 'sgd'},\n",
       "  'result': {'model': <keras.engine.sequential.Sequential at 0x20b96347848>,\n",
       "   'accuracy': 0.6582365003417635}},\n",
       " {'test_param_val': 4,\n",
       "  'params': {'input_layer_dim': 48,\n",
       "   'output_layer_dim': 11,\n",
       "   'number_of_hidden_layers': 4,\n",
       "   'number_of_units_per_layer': 48,\n",
       "   'epochs': 100,\n",
       "   'batch_size': 64,\n",
       "   'activation_function': 'relu',\n",
       "   'loss_function': 'mean_squared_error',\n",
       "   'optimizer': 'sgd'},\n",
       "  'result': {'model': <keras.engine.sequential.Sequential at 0x20b9f453b48>,\n",
       "   'accuracy': 0.6555023923444976}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnpt.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n",
      "[0.6582365003417635, 0.6555023923444976]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnpt.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
