{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"../data/Component_Faults_Data.csv\"\n",
    "df = pd.read_csv(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, :48].values\n",
    "y = df[\"class\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newer Solution for testing NN-Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "    \n",
    "class NeuralNetworkParameterTester:\n",
    "\n",
    "    # params for building model\n",
    "    config = {\n",
    "        'input_layer_dim': 48,\n",
    "        'output_layer_dim': 11,\n",
    "        'number_of_hidden_layers': 1,\n",
    "        'number_of_units_per_layer': 10,\n",
    "        'epochs': 100,\n",
    "        'batch_size': 64,\n",
    "        'activation_function': 'relu',\n",
    "        'loss_function': 'categorical_crossentropy',\n",
    "        'optimizer': 'sgd'}\n",
    "\n",
    "    \n",
    "    test_param_name = None  # Name of the div parameter\n",
    "    test_param_val = None  # Name of the div parameter\n",
    "    \n",
    "    result = []  # Result of the test\n",
    "    \n",
    "    # schema of result\n",
    "    \n",
    "\n",
    "    # [\n",
    "    #    [\n",
    "    #        'test_param_val': 1,\n",
    "    #        'params': [\n",
    "    #            'input_layer_dim': 48,\n",
    "    #            'output_layer_dim': 11,\n",
    "    #            'activation_function': 'relu',\n",
    "    #            'loss_function': 'mean_squared_error',\n",
    "    #            'optimizer': 'sgd',\n",
    "    #            'number_of_hidden_layers': 1,\n",
    "    #            'number_of_units_per_layer': 10]\n",
    "    #        'result': [\n",
    "    #            'model': model,\n",
    "    #            'acurracy': 0.923\n",
    "    #        ]\n",
    "    #    ],\n",
    "    #    [\n",
    "    #        'test_param_val': 2,\n",
    "    #        'params': [\n",
    "    #            'input_layer_dim': 48,\n",
    "    #            'output_layer_dim': 11,\n",
    "    #            'activation_function': 'relu',\n",
    "    #            'loss_function': 'mean_squared_error',\n",
    "    #            'optimizer': 'sgd',\n",
    "    #            'number_of_hidden_layers': 2,\n",
    "    #            'number_of_units_per_layer': 10]\n",
    "    #        'result': [\n",
    "    #            'model': model,\n",
    "    #            'acurracy': 0.923\n",
    "    #        ]\n",
    "    #    ],\n",
    "    #   [...........]\n",
    "    # ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, test_param_name, test_param_val):\n",
    "        \n",
    "        # 'param_name' and 'param_val' must be of right type and not None\n",
    "        # if this is the case, they will be set\n",
    "        \n",
    "        if test_param_name is not None and isinstance(test_param_name, str):\n",
    "            self.test_param_name = test_param_name\n",
    "            if test_param_val is not None and (isinstance(test_param_val, int) or isinstance(test_param_val, list)):\n",
    "                self.test_param_val = test_param_val\n",
    "                self.config[self.test_param_name] = self.test_param_val\n",
    "            else:\n",
    "                print(\"'test_param_val' must be of type int or list\")\n",
    "                sys.exit(0)\n",
    "        else:\n",
    "            print(\"'param_name' must be of type str\")\n",
    "            sys.exit(0)\n",
    "        #self.result.append({\"test_param_name\": self.test_param_name})\n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        for val in self.config[self.test_param_name]:\n",
    "            \n",
    "            print(val)\n",
    "            \n",
    "            # prepare config for individual test\n",
    "            test_config = self.config  \n",
    "            test_config[self.test_param_name] = val\n",
    "        \n",
    "            model = self.__build(test_config)\n",
    "            trained_model, history = self.__train(test_config, model)\n",
    "            accuracy = self.__test(trained_model)\n",
    "            \n",
    "            # save result\n",
    "            self.result.append({\"test_param_val\": val, \n",
    "                                'params': test_config.copy(), \n",
    "                                \"result\": {'model': trained_model, \n",
    "                                           'accuracy': accuracy}})\n",
    "\n",
    "\n",
    "    def __build(self, test_config):\n",
    "\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        \n",
    "        # Input layer\n",
    "        model.add(Dense(test_config['number_of_units_per_layer'], \n",
    "                        input_dim=test_config['input_layer_dim'], \n",
    "                        activation=test_config['activation_function']))\n",
    "        # Hidden layer\n",
    "        for i in range(test_config['number_of_hidden_layers']):\n",
    "            model.add(Dense(test_config['number_of_units_per_layer'], \n",
    "                            activation=test_config['activation_function']))\n",
    "        # Output layer\n",
    "        model.add(Dense(test_config['output_layer_dim'], \n",
    "                        activation=\"softmax\"))\n",
    "\n",
    "        model.compile(loss=test_config['loss_function'], \n",
    "                      optimizer=test_config['optimizer'], \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def __train(self, test_config, model):\n",
    "        # self.param_list.update({\"epochs\": epochs, \"batch_size\": batch_size})\n",
    "                                    \n",
    "        # xtrain and ytrain are from preprocessing\n",
    "        history = model.fit(x_train, \n",
    "                            y_train, \n",
    "                            epochs=test_config['epochs'], \n",
    "                            batch_size=test_config['batch_size'])\n",
    "                                    \n",
    "        return model, history\n",
    "    \n",
    "    @staticmethod\n",
    "    def __test(model):\n",
    "        y_pred = model.predict(x_test)\n",
    "        # Converting predictions to label\n",
    "        pred = list()\n",
    "        for i in range(len(y_pred)):\n",
    "            pred.append(np.argmax(y_pred[i]))\n",
    "        # Converting one hot encoded test label to label\n",
    "        test = list()\n",
    "        for i in range(len(y_test)):\n",
    "            test.append(np.argmax(y_test[i]))\n",
    "\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score(pred, test)\n",
    "        return accuracy\n",
    "                                    \n",
    "                                    \n",
    "    # Setter methods for setting single parameters\n",
    "    \n",
    "    def set_number_of_hidden_layers(self, val):\n",
    "        self.config[\"number_of_hidden_layers\"] = val\n",
    "        \n",
    "    def set_number_of_units_per_layer(self, val):\n",
    "        self.config[\"number_of_units_per_layer\"] = val\n",
    "        \n",
    "    def set_activation_function(self, val):\n",
    "        self.config[\"activation_function\"] = val\n",
    "        \n",
    "    def set_epochs(self, val):\n",
    "        self.config[\"epochs\"] = val\n",
    "        \n",
    "    def set_batch_size(self, val):\n",
    "        self.config[\"batch_size\"] = val\n",
    "        \n",
    "    def set_loss_function(self, val):\n",
    "        self.config[\"loss_function\"] = val\n",
    "        \n",
    "    def set_optimizer(self, val):\n",
    "        self.config[\"optimizer\"] = val\n",
    "        \n",
    "    def reset_config(self):\n",
    "        config = {\n",
    "            'input_layer_dim': 48,\n",
    "            'output_layer_dim': 11,\n",
    "            'number_of_hidden_layers': 1,\n",
    "            'number_of_units_per_layer': 10,\n",
    "            'epochs': 100,\n",
    "            'batch_size': 64,\n",
    "            'activation_function': 'relu',\n",
    "            'loss_function': 'mean_squared_error',\n",
    "            'optimizer': 'sgd'}\n",
    "        \n",
    "        config[self.test_param_name] = self.test_param_val\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "    # Plot result\n",
    "    \n",
    "    def plot_result(self, dim=2):\n",
    "        if dim == 2:\n",
    "            self.__plot_2d()\n",
    "\n",
    "    def __plot_2d(self):\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        test_param_val = [element['test_param_val'] for element in self.result]\n",
    "        accuracy_result = [element['result']['accuracy'] for element in self.result]\n",
    "        \n",
    "        print(test_param_val)\n",
    "        print(accuracy_result)\n",
    "\n",
    "        plt.plot(test_param_val, accuracy_result)\n",
    "        plt.title(\"Accuracy per \" + \"'\" + self.test_param_name + \"'\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(self.test_param_name)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NN with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_layer_dim': 48,\n",
       " 'output_layer_dim': 11,\n",
       " 'number_of_hidden_layers': [2, 4],\n",
       " 'number_of_units_per_layer': 48,\n",
       " 'epochs': 100,\n",
       " 'batch_size': 64,\n",
       " 'activation_function': 'relu',\n",
       " 'loss_function': 'categorical_crossentropy',\n",
       " 'optimizer': 'sgd'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnpt = NeuralNetworkParameterTester(test_param_name=\"number_of_hidden_layers\", test_param_val=[2, 4])\n",
    "nnpt.set_number_of_units_per_layer(48)\n",
    "nnpt.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Fab\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "26329/26329 [==============================] - 1s 38us/step - loss: 2.0531 - acc: 0.2642\n",
      "Epoch 2/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 1.4414 - acc: 0.4833\n",
      "Epoch 3/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 1.0568 - acc: 0.6071\n",
      "Epoch 4/100\n",
      "26329/26329 [==============================] - 0s 19us/step - loss: 0.8148 - acc: 0.6872\n",
      "Epoch 5/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.6295 - acc: 0.7634\n",
      "Epoch 6/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.4854 - acc: 0.8287\n",
      "Epoch 7/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.3849 - acc: 0.8704\n",
      "Epoch 8/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.3168 - acc: 0.8926\n",
      "Epoch 9/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2717 - acc: 0.9086\n",
      "Epoch 10/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2406 - acc: 0.9204\n",
      "Epoch 11/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.2164 - acc: 0.9278\n",
      "Epoch 12/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.1980 - acc: 0.9350\n",
      "Epoch 13/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.1830 - acc: 0.9410\n",
      "Epoch 14/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.1705 - acc: 0.9445\n",
      "Epoch 15/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.1599 - acc: 0.9480\n",
      "Epoch 16/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.1503 - acc: 0.9522\n",
      "Epoch 17/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.1419 - acc: 0.9548\n",
      "Epoch 18/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.1342 - acc: 0.9576\n",
      "Epoch 19/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.1272 - acc: 0.9597\n",
      "Epoch 20/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.1216 - acc: 0.9620\n",
      "Epoch 21/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.1162 - acc: 0.9633\n",
      "Epoch 22/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.1107 - acc: 0.9652\n",
      "Epoch 23/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.1066 - acc: 0.9668\n",
      "Epoch 24/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.1027 - acc: 0.9682\n",
      "Epoch 25/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0987 - acc: 0.9696\n",
      "Epoch 26/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0948 - acc: 0.9708\n",
      "Epoch 27/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0921 - acc: 0.9708\n",
      "Epoch 28/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0896 - acc: 0.9719\n",
      "Epoch 29/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0867 - acc: 0.9738\n",
      "Epoch 30/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0840 - acc: 0.9746\n",
      "Epoch 31/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0819 - acc: 0.9746\n",
      "Epoch 32/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0789 - acc: 0.9757\n",
      "Epoch 33/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0776 - acc: 0.9762\n",
      "Epoch 34/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0759 - acc: 0.9769\n",
      "Epoch 35/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0735 - acc: 0.9788\n",
      "Epoch 36/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0718 - acc: 0.9781\n",
      "Epoch 37/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0702 - acc: 0.9779\n",
      "Epoch 38/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0684 - acc: 0.9795\n",
      "Epoch 39/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0667 - acc: 0.9801\n",
      "Epoch 40/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0658 - acc: 0.9801\n",
      "Epoch 41/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0644 - acc: 0.9805\n",
      "Epoch 42/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0635 - acc: 0.9817\n",
      "Epoch 43/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0614 - acc: 0.9820\n",
      "Epoch 44/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0605 - acc: 0.9821\n",
      "Epoch 45/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0602 - acc: 0.9822\n",
      "Epoch 46/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0577 - acc: 0.9838\n",
      "Epoch 47/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0571 - acc: 0.9836\n",
      "Epoch 48/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0562 - acc: 0.9837\n",
      "Epoch 49/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0550 - acc: 0.9842\n",
      "Epoch 50/100\n",
      "26329/26329 [==============================] - 0s 19us/step - loss: 0.0543 - acc: 0.9851\n",
      "Epoch 51/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0537 - acc: 0.9840\n",
      "Epoch 52/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0521 - acc: 0.9848\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0514 - acc: 0.9852\n",
      "Epoch 54/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0502 - acc: 0.9863\n",
      "Epoch 55/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0500 - acc: 0.9856\n",
      "Epoch 56/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0497 - acc: 0.9857\n",
      "Epoch 57/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0484 - acc: 0.9864\n",
      "Epoch 58/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0473 - acc: 0.9869\n",
      "Epoch 59/100\n",
      "26329/26329 [==============================] - 0s 16us/step - loss: 0.0465 - acc: 0.9875\n",
      "Epoch 60/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0464 - acc: 0.9873\n",
      "Epoch 61/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0449 - acc: 0.9881\n",
      "Epoch 62/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0447 - acc: 0.9875\n",
      "Epoch 63/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0440 - acc: 0.9879\n",
      "Epoch 64/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0428 - acc: 0.9884\n",
      "Epoch 65/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0428 - acc: 0.9878\n",
      "Epoch 66/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0417 - acc: 0.9886\n",
      "Epoch 67/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0417 - acc: 0.9884\n",
      "Epoch 68/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0410 - acc: 0.9888\n",
      "Epoch 69/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0399 - acc: 0.9894\n",
      "Epoch 70/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0401 - acc: 0.9896\n",
      "Epoch 71/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0385 - acc: 0.9895\n",
      "Epoch 72/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0390 - acc: 0.9897\n",
      "Epoch 73/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0387 - acc: 0.9891\n",
      "Epoch 74/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0373 - acc: 0.9902\n",
      "Epoch 75/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0369 - acc: 0.9904\n",
      "Epoch 76/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0369 - acc: 0.9900\n",
      "Epoch 77/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0366 - acc: 0.9902\n",
      "Epoch 78/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0365 - acc: 0.9893\n",
      "Epoch 79/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0352 - acc: 0.9908\n",
      "Epoch 80/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0350 - acc: 0.9908\n",
      "Epoch 81/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0347 - acc: 0.9913\n",
      "Epoch 82/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0347 - acc: 0.9907\n",
      "Epoch 83/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0337 - acc: 0.9913\n",
      "Epoch 84/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0333 - acc: 0.9909\n",
      "Epoch 85/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0334 - acc: 0.9915\n",
      "Epoch 86/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0325 - acc: 0.9914\n",
      "Epoch 87/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0322 - acc: 0.9916\n",
      "Epoch 88/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0319 - acc: 0.9918\n",
      "Epoch 89/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0311 - acc: 0.9924\n",
      "Epoch 90/100\n",
      "26329/26329 [==============================] - 0s 18us/step - loss: 0.0317 - acc: 0.9918\n",
      "Epoch 91/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0307 - acc: 0.9919\n",
      "Epoch 92/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0306 - acc: 0.9923\n",
      "Epoch 93/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0301 - acc: 0.9924\n",
      "Epoch 94/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0298 - acc: 0.9930\n",
      "Epoch 95/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0296 - acc: 0.9922\n",
      "Epoch 96/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0292 - acc: 0.9929\n",
      "Epoch 97/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0293 - acc: 0.9930\n",
      "Epoch 98/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0291 - acc: 0.9923\n",
      "Epoch 99/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0284 - acc: 0.9926: 0s - loss: 0.0283 - acc: \n",
      "Epoch 100/100\n",
      "26329/26329 [==============================] - 0s 17us/step - loss: 0.0280 - acc: 0.9932\n",
      "4\n",
      "Epoch 1/100\n",
      "26329/26329 [==============================] - 1s 37us/step - loss: 2.3114 - acc: 0.1894\n",
      "Epoch 2/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 1.8228 - acc: 0.3329\n",
      "Epoch 3/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 1.0833 - acc: 0.5904\n",
      "Epoch 4/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.6855 - acc: 0.7341\n",
      "Epoch 5/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.4591 - acc: 0.8274\n",
      "Epoch 6/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.3412 - acc: 0.8760\n",
      "Epoch 7/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.2798 - acc: 0.8987\n",
      "Epoch 8/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.2406 - acc: 0.9121\n",
      "Epoch 9/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.2148 - acc: 0.9231\n",
      "Epoch 10/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.1910 - acc: 0.9330\n",
      "Epoch 11/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.1765 - acc: 0.9394\n",
      "Epoch 12/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.1621 - acc: 0.9453\n",
      "Epoch 13/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.1513 - acc: 0.9486\n",
      "Epoch 14/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.1401 - acc: 0.9534\n",
      "Epoch 15/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.1299 - acc: 0.9563\n",
      "Epoch 16/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.1247 - acc: 0.9585\n",
      "Epoch 17/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.1153 - acc: 0.9624\n",
      "Epoch 18/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.1129 - acc: 0.9623\n",
      "Epoch 19/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.1063 - acc: 0.9656\n",
      "Epoch 20/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.1011 - acc: 0.9671\n",
      "Epoch 21/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0974 - acc: 0.9689\n",
      "Epoch 22/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0945 - acc: 0.9687\n",
      "Epoch 23/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0881 - acc: 0.9717\n",
      "Epoch 24/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0850 - acc: 0.9731\n",
      "Epoch 25/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0843 - acc: 0.9730\n",
      "Epoch 26/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0788 - acc: 0.9755\n",
      "Epoch 27/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0752 - acc: 0.9772\n",
      "Epoch 28/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0724 - acc: 0.9769\n",
      "Epoch 29/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0714 - acc: 0.9770\n",
      "Epoch 30/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0708 - acc: 0.9774\n",
      "Epoch 31/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0683 - acc: 0.9782\n",
      "Epoch 32/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0659 - acc: 0.9801\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0634 - acc: 0.9796\n",
      "Epoch 34/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0615 - acc: 0.9809\n",
      "Epoch 35/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0615 - acc: 0.9807\n",
      "Epoch 36/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0581 - acc: 0.9817\n",
      "Epoch 37/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0549 - acc: 0.9836\n",
      "Epoch 38/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0530 - acc: 0.9842\n",
      "Epoch 39/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0523 - acc: 0.9836\n",
      "Epoch 40/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0521 - acc: 0.9837\n",
      "Epoch 41/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0485 - acc: 0.9849\n",
      "Epoch 42/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0491 - acc: 0.9857\n",
      "Epoch 43/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0474 - acc: 0.9851\n",
      "Epoch 44/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0465 - acc: 0.9856\n",
      "Epoch 45/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0455 - acc: 0.9861\n",
      "Epoch 46/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0464 - acc: 0.9865\n",
      "Epoch 47/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0441 - acc: 0.9870\n",
      "Epoch 48/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0425 - acc: 0.9876\n",
      "Epoch 49/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0417 - acc: 0.9875\n",
      "Epoch 50/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0410 - acc: 0.9881\n",
      "Epoch 51/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0414 - acc: 0.9880\n",
      "Epoch 52/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0391 - acc: 0.9881\n",
      "Epoch 53/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0402 - acc: 0.9879\n",
      "Epoch 54/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0370 - acc: 0.9890\n",
      "Epoch 55/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0363 - acc: 0.9892\n",
      "Epoch 56/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0369 - acc: 0.9890\n",
      "Epoch 57/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0360 - acc: 0.9894\n",
      "Epoch 58/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0338 - acc: 0.9903\n",
      "Epoch 59/100\n",
      "26329/26329 [==============================] - 1s 22us/step - loss: 0.0329 - acc: 0.9907\n",
      "Epoch 60/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0343 - acc: 0.9900\n",
      "Epoch 61/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0342 - acc: 0.9901\n",
      "Epoch 62/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0331 - acc: 0.9906\n",
      "Epoch 63/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0315 - acc: 0.9912\n",
      "Epoch 64/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0326 - acc: 0.9908\n",
      "Epoch 65/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0318 - acc: 0.9913\n",
      "Epoch 66/100\n",
      "26329/26329 [==============================] - 1s 23us/step - loss: 0.0289 - acc: 0.9923\n",
      "Epoch 67/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0294 - acc: 0.9916\n",
      "Epoch 68/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0290 - acc: 0.9916\n",
      "Epoch 69/100\n",
      "26329/26329 [==============================] - 1s 22us/step - loss: 0.0264 - acc: 0.9927\n",
      "Epoch 70/100\n",
      "26329/26329 [==============================] - 1s 24us/step - loss: 0.0281 - acc: 0.9926\n",
      "Epoch 71/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0271 - acc: 0.9926\n",
      "Epoch 72/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0282 - acc: 0.9921\n",
      "Epoch 73/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0280 - acc: 0.9919\n",
      "Epoch 74/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0260 - acc: 0.9932\n",
      "Epoch 75/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0255 - acc: 0.9933\n",
      "Epoch 76/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0268 - acc: 0.9929\n",
      "Epoch 77/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0267 - acc: 0.9926\n",
      "Epoch 78/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0272 - acc: 0.9927\n",
      "Epoch 79/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0241 - acc: 0.9934\n",
      "Epoch 80/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0238 - acc: 0.9937\n",
      "Epoch 81/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0246 - acc: 0.9932\n",
      "Epoch 82/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0244 - acc: 0.9935\n",
      "Epoch 83/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0214 - acc: 0.9950\n",
      "Epoch 84/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0256 - acc: 0.9930\n",
      "Epoch 85/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0233 - acc: 0.9937\n",
      "Epoch 86/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0231 - acc: 0.9940\n",
      "Epoch 87/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0211 - acc: 0.9947\n",
      "Epoch 88/100\n",
      "26329/26329 [==============================] - 1s 19us/step - loss: 0.0241 - acc: 0.9930\n",
      "Epoch 89/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0229 - acc: 0.9937\n",
      "Epoch 90/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0216 - acc: 0.9945\n",
      "Epoch 91/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0215 - acc: 0.9950\n",
      "Epoch 92/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0248 - acc: 0.9935\n",
      "Epoch 93/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0224 - acc: 0.9940\n",
      "Epoch 94/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0210 - acc: 0.9949\n",
      "Epoch 95/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0210 - acc: 0.9950\n",
      "Epoch 96/100\n",
      "26329/26329 [==============================] - 1s 21us/step - loss: 0.0209 - acc: 0.9949\n",
      "Epoch 97/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0223 - acc: 0.9949\n",
      "Epoch 98/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0213 - acc: 0.9943\n",
      "Epoch 99/100\n",
      "26329/26329 [==============================] - 1s 20us/step - loss: 0.0226 - acc: 0.9944\n",
      "Epoch 100/100\n",
      "26329/26329 [==============================] - 1s 22us/step - loss: 0.0194 - acc: 0.9949\n"
     ]
    }
   ],
   "source": [
    "nnpt.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'test_param_val': 2,\n",
       "  'params': {'input_layer_dim': 48,\n",
       "   'output_layer_dim': 11,\n",
       "   'number_of_hidden_layers': 2,\n",
       "   'number_of_units_per_layer': 48,\n",
       "   'epochs': 100,\n",
       "   'batch_size': 64,\n",
       "   'activation_function': 'relu',\n",
       "   'loss_function': 'categorical_crossentropy',\n",
       "   'optimizer': 'sgd'},\n",
       "  'result': {'model': <keras.engine.sequential.Sequential at 0x2012d684948>,\n",
       "   'accuracy': 0.987012987012987}},\n",
       " {'test_param_val': 4,\n",
       "  'params': {'input_layer_dim': 48,\n",
       "   'output_layer_dim': 11,\n",
       "   'number_of_hidden_layers': 4,\n",
       "   'number_of_units_per_layer': 48,\n",
       "   'epochs': 100,\n",
       "   'batch_size': 64,\n",
       "   'activation_function': 'relu',\n",
       "   'loss_function': 'categorical_crossentropy',\n",
       "   'optimizer': 'sgd'},\n",
       "  'result': {'model': <keras.engine.sequential.Sequential at 0x20138983688>,\n",
       "   'accuracy': 0.9863294600136705}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnpt.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n",
      "[0.987012987012987, 0.9863294600136705]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnpt.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot Matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
